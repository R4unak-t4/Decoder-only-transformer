{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "5f225e11d40a40109e3a6a27707f9710": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_25f7d357afda4afb978045510e8716dd",
       "IPY_MODEL_e91681c138034f23bc3ff50ce6082a1c",
       "IPY_MODEL_f9b8f30d533b4564bf47d783a3fcc6a8"
      ],
      "layout": "IPY_MODEL_9d7fefca34fd4237afff1f190ca03793"
     }
    },
    "25f7d357afda4afb978045510e8716dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65c1d4f2697b40748a57918251c841cf",
      "placeholder": "​",
      "style": "IPY_MODEL_ed19207a7bc04688b500546a7630cfc0",
      "value": "Epoch 29: 100%"
     }
    },
    "e91681c138034f23bc3ff50ce6082a1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ec9f45420b84a96896b48ac912681c4",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c242fc8ea1004131b684e0988245dde5",
      "value": 2
     }
    },
    "f9b8f30d533b4564bf47d783a3fcc6a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a52c75a6a6894b20a624497590a6fc16",
      "placeholder": "​",
      "style": "IPY_MODEL_07c3250e755642a88fe2d08f0ec8d26e",
      "value": " 2/2 [00:00&lt;00:00, 105.66it/s, v_num=2]"
     }
    },
    "9d7fefca34fd4237afff1f190ca03793": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "65c1d4f2697b40748a57918251c841cf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed19207a7bc04688b500546a7630cfc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ec9f45420b84a96896b48ac912681c4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c242fc8ea1004131b684e0988245dde5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a52c75a6a6894b20a624497590a6fc16": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "07c3250e755642a88fe2d08f0ec8d26e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BRmn5Kaq0ckx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Training components"
   ],
   "metadata": {
    "id": "wOh4TmwP7aEq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "token_to_id = {\n",
    "    'what' : 0,\n",
    "    'is' : 1,\n",
    "    'transformer' : 2,\n",
    "    'magic' : 3,\n",
    "    '<EOS>' : 4\n",
    "}\n",
    "\n",
    "id_to_token = dict(map(reversed, token_to_id.items()))"
   ],
   "metadata": {
    "id": "N0VLibRI42Mf"
   },
   "execution_count": 84,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "inputs = torch.tensor([[token_to_id[\"what\"],\n",
    "                        token_to_id[\"is\"],\n",
    "                        token_to_id[\"transformer\"],\n",
    "                        token_to_id[\"<EOS>\"],\n",
    "                        token_to_id[\"magic\"]],\n",
    "\n",
    "                       [token_to_id[\"transformer\"],\n",
    "                        token_to_id[\"is\"],\n",
    "                        token_to_id[\"what\"],\n",
    "                        token_to_id[\"<EOS>\"],\n",
    "                        token_to_id[\"magic\"]]])\n",
    "\n",
    "labels = torch.tensor([[token_to_id[\"is\"],\n",
    "                        token_to_id[\"transformer\"],\n",
    "                        token_to_id[\"<EOS>\"],\n",
    "                        token_to_id[\"magic\"],\n",
    "                        token_to_id[\"<EOS>\"]],\n",
    "\n",
    "                       [token_to_id[\"is\"],\n",
    "                        token_to_id[\"what\"],\n",
    "                        token_to_id[\"<EOS>\"],\n",
    "                        token_to_id[\"magic\"],\n",
    "                        token_to_id[\"<EOS>\"]]])"
   ],
   "metadata": {
    "id": "t8bZQRdV6Gfd"
   },
   "execution_count": 85,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)"
   ],
   "metadata": {
    "id": "ls2zBUwQ_CCZ"
   },
   "execution_count": 86,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# position encoding\n",
    "We'll use Sinusoidal Positional Encoding\n",
    "\n",
    "Given a position \\( pos \\) and dimension \\( i \\), the positional encoding is defined as:\n",
    "\n",
    "$$\n",
    "\\text{PE}(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{PE}(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "\n"
   ],
   "metadata": {
    "id": "AwjIzoOU_HPh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "    def __init__(self, d_model=2, max_len=6):\n",
    "      super().__init__()\n",
    "      position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "\n",
    "      embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "\n",
    "      div_term = 1/torch.tensor(10000.0)**(embedding_index / d_model)\n",
    "\n",
    "      pe = torch.zeros(max_len, d_model)\n",
    "      pe[:, 0::2] = torch.sin(position * div_term)\n",
    "      pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "      self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, word_embeddings):\n",
    "      return word_embeddings + self.pe[:word_embeddings.size(0), :]\n"
   ],
   "metadata": {
    "id": "txy0odEd_G65"
   },
   "execution_count": 110,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Attention Mechanism\n",
    "* Q,K,V values"
   ],
   "metadata": {
    "id": "nnw1j7uuoB7a"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Attention(nn.Module):\n",
    "  def __init__(self, d_model=2):\n",
    "    super().__init__()\n",
    "\n",
    "    self.W_q = nn.Linear(in_features= d_model, out_features=d_model, bias=False)\n",
    "    self.W_k = nn.Linear(in_features= d_model, out_features=d_model, bias=False)\n",
    "    self.W_v = nn.Linear(in_features= d_model, out_features=d_model, bias=False)\n",
    "\n",
    "    self.row_dim=0\n",
    "    self.col_dim=1\n",
    "\n",
    "  def forward(self,encoding_for_q, encoding_for_k, encoding_for_v,mask=None):\n",
    "\n",
    "    q = self.W_q(encoding_for_q)\n",
    "    k = self.W_k(encoding_for_k)\n",
    "    v = self.W_v(encoding_for_v)\n",
    "\n",
    "    sims = torch.matmul(q,k.transpose(dim0=self.row_dim,dim1=self.col_dim))\n",
    "\n",
    "    #normalization\n",
    "    scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
    "\n",
    "    if mask is not None:\n",
    "      scaled_sims = scaled_sims.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    attention_weights = F.softmax(scaled_sims, dim=1)\n",
    "\n",
    "    #returning attention score\n",
    "    return torch.matmul(attention_weights,v)"
   ],
   "metadata": {
    "id": "hnXEomQ4_CXT"
   },
   "execution_count": 111,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.Decoder only transformer"
   ],
   "metadata": {
    "id": "yFxtje8hsbBG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install lightning"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "KYDDUATQsijt",
    "outputId": "a88d2c79-4f55-4b10-a428-11ba8ddc2374"
   },
   "execution_count": 89,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: lightning in /usr/local/lib/python3.11/dist-packages (2.5.1.post0)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning) (6.0.2)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2025.3.2)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (0.14.3)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (24.2)\n",
      "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (1.7.3)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (4.14.0)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from lightning) (2.5.1.post0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.15)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.18.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (2.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import lightning as l"
   ],
   "metadata": {
    "id": "tw4ubJjhtT3t"
   },
   "execution_count": 90,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class DecoderOnlyTransformer(l.LightningModule):\n",
    "    def __init__(self, num_tokens=4, d_model=2, max_len=6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)\n",
    "        self.pe = PositionEncoding(d_model=d_model, max_len=max_len)\n",
    "        self.attention = Attention(d_model=d_model)\n",
    "        self.fc = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        word_embeddings = self.we(token_ids)\n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "\n",
    "        mask = torch.tril(torch.ones((token_ids.size(dim=0), token_ids.size(dim=0)), device=self.device))\n",
    "        mask = mask == 0\n",
    "\n",
    "        attention_values = self.attention(position_encoded, position_encoded, position_encoded, mask=mask)\n",
    "        residual_connection_values = attention_values + position_encoded\n",
    "        logits = self.fc(residual_connection_values)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_tokens, labels = batch\n",
    "        output = self.forward(input_tokens[0])\n",
    "        loss = self.loss(output.view(-1, output.size(-1)), labels[0].view(-1))\n",
    "        return loss"
   ],
   "metadata": {
    "id": "-bsGgRmOt1Oe"
   },
   "execution_count": 115,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Running Before training"
   ],
   "metadata": {
    "id": "C8Uel6kAwrjE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = DecoderOnlyTransformer(num_tokens=len(token_to_id), d_model=2, max_len=6)"
   ],
   "metadata": {
    "id": "6YNvWbJJvzQe"
   },
   "execution_count": 116,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_input = torch.tensor([token_to_id[\"what\"],\n",
    "                            token_to_id[\"is\"],\n",
    "                            token_to_id[\"transformer\"],\n",
    "                            token_to_id[\"<EOS>\"]])\n",
    "input_length = model_input.size(dim=0)\n",
    "\n",
    "predictions = model(model_input)\n",
    "predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "predicted_ids = predicted_id\n",
    "\n",
    "max_length = 6\n",
    "for i in range(input_length, max_length):\n",
    "    if (predicted_id == token_to_id[\"<EOS>\"]):\n",
    "        break\n",
    "\n",
    "    model_input = torch.cat((model_input, predicted_id))\n",
    "\n",
    "    predictions = model(model_input)\n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "\n",
    "print(\"Predicted Tokens:\\n\")\n",
    "for id in predicted_ids:\n",
    "    print(\"\\t\", id_to_token[id.item()])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JIkGEiD1v-Wb",
    "outputId": "97d724fb-16e7-45d7-a15e-350f776f779c"
   },
   "execution_count": 93,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted Tokens:\n",
      "\n",
      "\t <EOS>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Traning the model"
   ],
   "metadata": {
    "id": "pUWFoCItx1y7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = l.Trainer(max_epochs=30)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853,
     "referenced_widgets": [
      "5f225e11d40a40109e3a6a27707f9710",
      "25f7d357afda4afb978045510e8716dd",
      "e91681c138034f23bc3ff50ce6082a1c",
      "f9b8f30d533b4564bf47d783a3fcc6a8",
      "9d7fefca34fd4237afff1f190ca03793",
      "65c1d4f2697b40748a57918251c841cf",
      "ed19207a7bc04688b500546a7630cfc0",
      "7ec9f45420b84a96896b48ac912681c4",
      "c242fc8ea1004131b684e0988245dde5",
      "a52c75a6a6894b20a624497590a6fc16",
      "07c3250e755642a88fe2d08f0ec8d26e"
     ]
    },
    "id": "ezriz1MOxYhk",
    "outputId": "58831bfb-4fa6-4496-a23d-7d9b279d7647"
   },
   "execution_count": 117,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO: Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | we        | Embedding        | 10     | train\n",
      "1 | pe        | PositionEncoding | 0      | train\n",
      "2 | attention | Attention        | 12     | train\n",
      "3 | fc        | Linear           | 15     | train\n",
      "4 | loss      | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "37        Trainable params\n",
      "0         Non-trainable params\n",
      "37        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | we        | Embedding        | 10     | train\n",
      "1 | pe        | PositionEncoding | 0      | train\n",
      "2 | attention | Attention        | 12     | train\n",
      "3 | fc        | Linear           | 15     | train\n",
      "4 | loss      | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "37        Trainable params\n",
      "0         Non-trainable params\n",
      "37        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f225e11d40a40109e3a6a27707f9710"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Running the model after training"
   ],
   "metadata": {
    "id": "Zna26AcT9AR4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_input = torch.tensor([token_to_id[\"what\"],\n",
    "                            token_to_id[\"is\"],\n",
    "                            token_to_id[\"transformer\"],\n",
    "                            token_to_id[\"<EOS>\"]])\n",
    "input_length = model_input.size(dim=0)\n",
    "\n",
    "predictions = model(model_input)\n",
    "predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "predicted_ids = predicted_id\n",
    "\n",
    "max_length = 6\n",
    "for i in range(input_length, max_length):\n",
    "    if (predicted_id == token_to_id[\"<EOS>\"]):\n",
    "        break\n",
    "\n",
    "    model_input = torch.cat((model_input, predicted_id))\n",
    "\n",
    "    predictions = model(model_input)\n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "\n",
    "print(\"Predicted Tokens:\\n\")\n",
    "for id in predicted_ids:\n",
    "    print(\"\\t\", id_to_token[id.item()])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1mspTPL1yE_a",
    "outputId": "1b2669d4-2b3c-4ba0-bbc8-c6519475f2f8"
   },
   "execution_count": 118,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted Tokens:\n",
      "\n",
      "\t magic\n",
      "\t <EOS>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Now that the model gives the correct token as prediction we'll try it for different token sequence"
   ],
   "metadata": {
    "id": "uYXATLF29HEp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_input = torch.tensor([\n",
    "                            token_to_id[\"transformer\"],\n",
    "                            token_to_id[\"is\"],\n",
    "                            token_to_id[\"what\"],\n",
    "                            token_to_id[\"<EOS>\"]])\n",
    "input_length = model_input.size(dim=0)\n",
    "\n",
    "predictions = model(model_input)\n",
    "predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "predicted_ids = predicted_id\n",
    "\n",
    "max_length = 6\n",
    "for i in range(input_length, max_length):\n",
    "    if (predicted_id == token_to_id[\"<EOS>\"]):\n",
    "        break\n",
    "\n",
    "    model_input = torch.cat((model_input, predicted_id))\n",
    "\n",
    "    predictions = model(model_input)\n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "\n",
    "print(\"Predicted Tokens:\\n\")\n",
    "for id in predicted_ids:\n",
    "    print(\"\\t\", id_to_token[id.item()])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpwv9y8u9GEw",
    "outputId": "0d7d5954-6023-4f76-97d5-7cf2ee3299a3"
   },
   "execution_count": 119,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted Tokens:\n",
      "\n",
      "\t magic\n",
      "\t <EOS>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "sj-5NAnz9lHn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e19a051a"
   },
   "source": [
    "## Technical Summary of the Decoder-Only Transformer\n",
    "\n",
    "This notebook implements a simplified decoder-only transformer model using PyTorch and Lightning. The model is designed for a small vocabulary and fixed sequence length, demonstrating the core components of a transformer architecture.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1.  **Tokenization and Embedding:**\n",
    "    *   A fixed vocabulary is defined with a mapping from tokens to IDs (`token_to_id`) and vice-versa (`id_to_token`).\n",
    "    *   An `nn.Embedding` layer (`self.we`) converts input token IDs into dense word embeddings of a specified dimension (`d_model`).\n",
    "\n",
    "2.  **Positional Encoding:**\n",
    "    *   A `PositionEncoding` module adds positional information to the word embeddings, as transformers do not inherently process sequential order.\n",
    "    *   This implementation uses a sinusoidal positional encoding scheme, where different dimensions of the encoding correspond to sinusoids of varying frequencies. The positional encodings are pre-calculated and added to the word embeddings.\n",
    "\n",
    "3.  **Attention Mechanism:**\n",
    "    *   An `Attention` module implements the scaled dot-product attention mechanism.\n",
    "    *   It calculates Query (Q), Key (K), and Value (V) vectors from the input using linear transformations (`self.q`, `self.k`, `self.v`).\n",
    "    *   Attention weights are computed by taking the dot product of Q and K, scaling by the square root of the model dimension (`d_model`), and applying a softmax function.\n",
    "    *   A causal mask is applied to the attention weights during training to prevent the model from attending to future tokens in the sequence, which is characteristic of decoder-only models.\n",
    "    *   The output of the attention layer is a weighted sum of the Value vectors, where the weights are the attention weights.\n",
    "\n",
    "4.  **Decoder-Only Architecture:**\n",
    "    *   The `DecoderOnlyTransformer` class combines the embedding, positional encoding, and attention layers.\n",
    "    *   The `forward` method takes a sequence of token IDs as input.\n",
    "    *   It first obtains word embeddings and adds positional encodings.\n",
    "    *   Then, it passes the position-encoded embeddings through the attention layer with a causal mask.\n",
    "    *   A residual connection is applied by adding the attention output to the position-encoded input.\n",
    "    *   Finally, a linear layer (`self.fc`) projects the output of the attention mechanism to the size of the vocabulary, producing logits for the next token prediction.\n",
    "\n",
    "5.  **Training with PyTorch Lightning:**\n",
    "    *   The model is wrapped in a PyTorch Lightning module, providing structure for training.\n",
    "    *   The `configure_optimizers` method defines the Adam optimizer.\n",
    "    *   The `training_step` method calculates the cross-entropy loss between the model's output logits and the target labels.\n",
    "    *   A Lightning `Trainer` is used to handle the training loop, including optimization and loss calculation over epochs.\n",
    "\n",
    "**Limitations of this Implementation:**\n",
    "\n",
    "*   **Simplified Attention:** This implementation uses a single attention head and does not include multi-head attention.\n",
    "*   **No Layer Normalization or Feed-Forward Networks:** Standard transformer blocks typically include layer normalization and feed-forward neural networks after the attention mechanism, which are omitted here for simplicity.\n",
    "*   **Small Scale:** The model and vocabulary size are very small, suitable for demonstrating the architecture but not for practical natural language processing tasks.\n",
    "*   **Basic Training Loop:** The training loop is minimal and does not include validation or advanced logging as typically found in more complete Lightning examples."
   ]
  }
 ]
}
